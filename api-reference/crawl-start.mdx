---
title: 'Start Crawl'
description: 'Trigger a new crawl for a domain'
api: 'POST /v1/crawl/start'
---

## Start Crawl

Initiate a new crawl for a verified domain.

<ParamField header="Authorization" type="string" required>
  Bearer token with your API key
</ParamField>

<ParamField body="domainId" type="string" required>
  The UUID of the verified domain to crawl
</ParamField>

<ParamField body="options" type="object">
  Scan options for the crawl
  
  <Expandable title="Options">
    <ParamField body="sslCheck" type="boolean" default="false">
      Check SSL certificate validity and expiration
    </ParamField>
    <ParamField body="seoAnalysis" type="boolean" default="false">
      Run SEO health checks on each page
    </ParamField>
    <ParamField body="anchorTextAnalysis" type="boolean" default="false">
      Analyze anchor text quality (Pro/Agency only)
    </ParamField>
    <ParamField body="responseTrends" type="boolean" default="false">
      Track response time trends (Pro/Agency only)
    </ParamField>
  </Expandable>
</ParamField>

<ParamField body="settings" type="object">
  Crawl configuration settings
  
  <Expandable title="Settings">
    <ParamField body="depth" type="integer" default="plan_default">
      Maximum crawl depth (subject to plan limits)
    </ParamField>
    <ParamField body="maxUrls" type="integer" default="plan_default">
      Maximum URLs to crawl (subject to plan limits)
    </ParamField>
    <ParamField body="startUrl" type="string" default="/">
      Custom start URL path
    </ParamField>
  </Expandable>
</ParamField>

### Request

```bash
curl -X POST \
  "https://api.seocrawler.app/v1/crawl/start" \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "domainId": "123e4567-e89b-12d3-a456-426614174000",
    "options": {
      "sslCheck": true,
      "seoAnalysis": true,
      "anchorTextAnalysis": false,
      "responseTrends": false
    },
    "settings": {
      "depth": 5,
      "maxUrls": 1000
    }
  }'
```

### Response

<ResponseField name="crawlId" type="string">
  UUID of the started crawl
</ResponseField>

<ResponseField name="status" type="string">
  Will be `started` or `queued`
</ResponseField>

<ResponseField name="domain" type="string">
  The domain being crawled
</ResponseField>

<ResponseField name="options" type="object">
  The scan options applied to this crawl
</ResponseField>

<ResponseField name="settings" type="object">
  The crawl settings applied
</ResponseField>

<ResponseField name="started_at" type="string">
  ISO 8601 timestamp when crawl started
</ResponseField>

<ResponseExample>
```json
{
  "crawlId": "456e4567-e89b-12d3-a456-426614174100",
  "status": "started",
  "domain": "example.com",
  "options": {
    "sslCheck": true,
    "seoAnalysis": true,
    "anchorTextAnalysis": false,
    "responseTrends": false
  },
  "settings": {
    "depth": 5,
    "maxUrls": 1000,
    "startUrl": "/"
  },
  "started_at": "2024-12-02T15:00:00Z"
}
```
</ResponseExample>

### Queued Response

If another crawl is running, the new crawl is queued:

```json
{
  "crawlId": "456e4567-e89b-12d3-a456-426614174100",
  "status": "queued",
  "position": 2,
  "estimated_start": "2024-12-02T15:10:00Z",
  "domain": "example.com"
}
```

## Code Examples

<Tabs>
  <Tab title="JavaScript">
    ```javascript
    async function startCrawl(domainId, options = {}) {
      const response = await fetch('https://api.seocrawler.app/v1/crawl/start', {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${process.env.SEO_CRAWLER_API_KEY}`,
          'Content-Type': 'application/json'
        },
        body: JSON.stringify({
          domainId,
          options: {
            sslCheck: true,
            seoAnalysis: true,
            ...options
          }
        })
      });
      
      if (!response.ok) {
        throw new Error(`Crawl start failed: ${response.status}`);
      }
      
      return response.json();
    }
    
    // Usage
    const crawl = await startCrawl('123e4567-e89b-12d3-a456-426614174000');
    console.log(`Crawl started: ${crawl.crawlId}`);
    ```
  </Tab>
  
  <Tab title="Python">
    ```python
    import requests
    import os
    
    def start_crawl(domain_id, options=None):
        if options is None:
            options = {
                'sslCheck': True,
                'seoAnalysis': True
            }
        
        response = requests.post(
            'https://api.seocrawler.app/v1/crawl/start',
            headers={
                'Authorization': f'Bearer {os.environ["SEO_CRAWLER_API_KEY"]}',
                'Content-Type': 'application/json'
            },
            json={
                'domainId': domain_id,
                'options': options
            }
        )
        
        response.raise_for_status()
        return response.json()
    
    # Usage
    crawl = start_crawl('123e4567-e89b-12d3-a456-426614174000')
    print(f"Crawl started: {crawl['crawlId']}")
    ```
  </Tab>
  
  <Tab title="PHP">
    ```php
    <?php
    function startCrawl($domainId, $options = []) {
        $defaultOptions = [
            'sslCheck' => true,
            'seoAnalysis' => true,
        ];
        
        $ch = curl_init();
        
        curl_setopt($ch, CURLOPT_URL, 'https://api.seocrawler.app/v1/crawl/start');
        curl_setopt($ch, CURLOPT_POST, true);
        curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
        curl_setopt($ch, CURLOPT_HTTPHEADER, [
            'Authorization: Bearer ' . getenv('SEO_CRAWLER_API_KEY'),
            'Content-Type: application/json'
        ]);
        curl_setopt($ch, CURLOPT_POSTFIELDS, json_encode([
            'domainId' => $domainId,
            'options' => array_merge($defaultOptions, $options)
        ]));
        
        $response = curl_exec($ch);
        $httpCode = curl_getinfo($ch, CURLINFO_HTTP_CODE);
        curl_close($ch);
        
        if ($httpCode !== 200) {
            throw new Exception("Crawl start failed: $httpCode");
        }
        
        return json_decode($response, true);
    }
    
    // Usage
    $crawl = startCrawl('123e4567-e89b-12d3-a456-426614174000');
    echo "Crawl started: " . $crawl['crawlId'];
    ```
  </Tab>
</Tabs>

## Error Responses

| Status | Code | Description |
|--------|------|-------------|
| `400` | `invalid_domain_id` | Domain ID format is invalid |
| `403` | `domain_not_verified` | Domain is not verified |
| `404` | `domain_not_found` | Domain doesn't exist |
| `429` | `daily_limit_reached` | Daily crawl limit exceeded |
| `422` | `option_not_available` | Scan option not available on your plan |

### Error Example

```json
{
  "error": {
    "code": "daily_limit_reached",
    "message": "You have reached your daily crawl limit of 10 crawls. Limit resets at midnight UTC.",
    "limit": 10,
    "used": 10,
    "resets_at": "2024-12-03T00:00:00Z"
  }
}
```

## Plan Limits Applied

The API respects your plan limits:

| Setting | Free | Solo | Pro | Agency |
|---------|------|------|-----|--------|
| Max depth | 2 | 5 | 10 | Unlimited |
| Max URLs | 100 | 1,000 | 10,000 | Unlimited |
| seoAnalysis | ❌ | ✅ | ✅ | ✅ |
| anchorTextAnalysis | ❌ | ❌ | ✅ | ✅ |
| responseTrends | ❌ | ❌ | ✅ | ✅ |

<Note>
  If you request settings above your plan limits, they'll be capped to your maximum allowed values.
</Note>

## Webhook Notification

Optionally receive a webhook when the crawl completes:

```json
{
  "domainId": "123e4567-e89b-12d3-a456-426614174000",
  "options": {
    "sslCheck": true
  },
  "webhook": {
    "url": "https://your-server.com/webhooks/crawl-complete",
    "secret": "your-webhook-secret"
  }
}
```

See [Webhooks](/integrations/webhooks) for more details.

