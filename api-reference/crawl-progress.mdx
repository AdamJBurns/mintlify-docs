---
title: 'Crawl Progress'
description: 'Monitor the progress of a running crawl'
api: 'GET /v1/crawl/{crawlId}/progress'
---

## Get Crawl Progress

Retrieve the current progress of a crawl.

<ParamField header="Authorization" type="string" required>
  Bearer token with your API key
</ParamField>

<ParamField path="crawlId" type="string" required>
  The UUID of the crawl to check
</ParamField>

### Request

```bash
curl -X GET \
  "https://api.seocrawler.app/v1/crawl/456e4567-e89b-12d3-a456-426614174100/progress" \
  -H "Authorization: Bearer YOUR_API_KEY"
```

### Response

<ResponseField name="crawlId" type="string">
  The crawl identifier
</ResponseField>

<ResponseField name="status" type="string">
  Current status: `queued`, `running`, `completed`, `failed`, `cancelled`
</ResponseField>

<ResponseField name="progress" type="object">
  Progress metrics (only present when running)
  
  <Expandable title="Progress Fields">
    <ResponseField name="discovered" type="integer">
      Total URLs discovered
    </ResponseField>
    <ResponseField name="checked" type="integer">
      URLs fully processed
    </ResponseField>
    <ResponseField name="remaining" type="integer">
      URLs in queue
    </ResponseField>
    <ResponseField name="percentage" type="number">
      Completion percentage (0-100)
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="stats" type="object">
  Running statistics
  
  <Expandable title="Stats Fields">
    <ResponseField name="errors" type="integer">
      Broken links found so far
    </ResponseField>
    <ResponseField name="redirects" type="integer">
      Redirects detected
    </ResponseField>
    <ResponseField name="success" type="integer">
      Successful responses
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseField name="timing" type="object">
  Timing information
  
  <Expandable title="Timing Fields">
    <ResponseField name="started_at" type="string">
      ISO 8601 timestamp when crawl started
    </ResponseField>
    <ResponseField name="elapsed_seconds" type="integer">
      Seconds since crawl started
    </ResponseField>
    <ResponseField name="estimated_remaining" type="integer">
      Estimated seconds remaining (null if unknown)
    </ResponseField>
  </Expandable>
</ResponseField>

<ResponseExample>
```json
{
  "crawlId": "456e4567-e89b-12d3-a456-426614174100",
  "domain": "example.com",
  "status": "running",
  "progress": {
    "discovered": 847,
    "checked": 534,
    "remaining": 313,
    "percentage": 63.0
  },
  "stats": {
    "errors": 8,
    "redirects": 23,
    "success": 503
  },
  "timing": {
    "started_at": "2024-12-02T15:00:00Z",
    "elapsed_seconds": 154,
    "estimated_remaining": 91
  }
}
```
</ResponseExample>

## Status Values

| Status | Description |
|--------|-------------|
| `queued` | Waiting for another crawl to complete |
| `running` | Actively crawling |
| `completed` | Finished successfully |
| `failed` | Encountered unrecoverable error |
| `cancelled` | Manually cancelled by user |

### Queued Response

```json
{
  "crawlId": "456e4567-e89b-12d3-a456-426614174100",
  "domain": "example.com",
  "status": "queued",
  "queue_position": 2,
  "estimated_start": "2024-12-02T15:10:00Z"
}
```

### Completed Response

```json
{
  "crawlId": "456e4567-e89b-12d3-a456-426614174100",
  "domain": "example.com",
  "status": "completed",
  "progress": {
    "discovered": 847,
    "checked": 847,
    "remaining": 0,
    "percentage": 100.0
  },
  "stats": {
    "errors": 12,
    "redirects": 45,
    "success": 790
  },
  "timing": {
    "started_at": "2024-12-02T15:00:00Z",
    "completed_at": "2024-12-02T15:04:23Z",
    "elapsed_seconds": 263
  },
  "results_url": "/v1/crawl/456e4567-e89b-12d3-a456-426614174100/results"
}
```

### Failed Response

```json
{
  "crawlId": "456e4567-e89b-12d3-a456-426614174100",
  "domain": "example.com",
  "status": "failed",
  "error": {
    "code": "connection_failed",
    "message": "Unable to connect to domain. DNS resolution failed.",
    "failed_at": "2024-12-02T15:01:23Z"
  },
  "timing": {
    "started_at": "2024-12-02T15:00:00Z",
    "failed_at": "2024-12-02T15:01:23Z",
    "elapsed_seconds": 83
  }
}
```

## Polling for Progress

Recommended polling approach:

```javascript
async function waitForCrawlComplete(crawlId, pollInterval = 5000) {
  while (true) {
    const response = await fetch(
      `https://api.seocrawler.app/v1/crawl/${crawlId}/progress`,
      {
        headers: {
          'Authorization': `Bearer ${process.env.SEO_CRAWLER_API_KEY}`
        }
      }
    );
    
    const data = await response.json();
    
    console.log(`Progress: ${data.progress?.percentage || 0}%`);
    
    if (data.status === 'completed') {
      return data;
    }
    
    if (data.status === 'failed') {
      throw new Error(`Crawl failed: ${data.error?.message}`);
    }
    
    if (data.status === 'cancelled') {
      throw new Error('Crawl was cancelled');
    }
    
    // Wait before next poll
    await new Promise(resolve => setTimeout(resolve, pollInterval));
  }
}

// Usage
const result = await waitForCrawlComplete('456e4567-e89b-12d3-a456-426614174100');
console.log(`Crawl complete! Found ${result.stats.errors} errors.`);
```

<Tip>
  Instead of polling, consider using [webhooks](/integrations/webhooks) for crawl completion notifications.
</Tip>

## Cancel Crawl

Cancel a running or queued crawl.

```
POST /v1/crawl/{crawlId}/cancel
```

### Request

```bash
curl -X POST \
  "https://api.seocrawler.app/v1/crawl/456e4567-e89b-12d3-a456-426614174100/cancel" \
  -H "Authorization: Bearer YOUR_API_KEY"
```

### Response

```json
{
  "crawlId": "456e4567-e89b-12d3-a456-426614174100",
  "status": "cancelled",
  "cancelled_at": "2024-12-02T15:03:00Z",
  "progress": {
    "discovered": 534,
    "checked": 420,
    "remaining": 114,
    "percentage": 78.7
  }
}
```

<Note>
  Cancelled crawls still return partial results for URLs that were checked before cancellation.
</Note>

## Error Responses

| Status | Code | Description |
|--------|------|-------------|
| `404` | `crawl_not_found` | Crawl doesn't exist |
| `400` | `crawl_not_running` | Cannot cancel completed/failed crawl |

